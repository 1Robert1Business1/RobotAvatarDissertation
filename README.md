For my MSc dissertation, I developed a personalised robot avatar capable of real-time facial and speech synchronisation, with the goal of exploring more natural and engaging humanâ€“machine interaction. The project combined computer vision, natural language processing, and 3D modelling into a single interactive system.

I used photogrammetry to build a realistic 3D facial model and implemented blendshapes in Unity to animate expressions dynamically. For speech and dialogue, I integrated large language model APIs to generate unscripted, context-aware responses, while synchronising facial movements with audio output to create lifelike interaction. I built the backend logic in Python, managing data pipelines, API integration, and response control.

The project demonstrated how advanced AI can be combined with expressive visual design to create engaging, responsive avatars. It was awarded a Distinction, and it highlighted my ability to deliver a complex, multidisciplinary project from concept through to deployment, with both technical and user-focused outcomes.

Some Images as well as the Unity software aspect have not been included due to Privacy reasons, but the results can still be produced by replacing certain images. For explanation of the work, seek out the dissertation PDF.
